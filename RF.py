# -*- coding: utf-8 -*-
"""Cópia de ATUAL VGG16 trabalhodeeplearningRF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17-LHYBF8J0XFWRyzELL5Lj5OHV8C06hz
"""

pip install spafe

import os
import numpy as np
from numpy import sum,isrealobj,sqrt
from numpy.random import standard_normal
import os
from sklearn.model_selection import train_test_split
from spafe.features.lfcc import lfcc
import spafe.utils.vis as vis
from scipy.signal import get_window
import scipy.fftpack as fft
from scipy import signal
import matplotlib.pyplot as plt
from datetime import date
from tqdm import tqdm
from loading_functions import *

from feat_gen_functions import *

from google.colab import drive
import os

# Monta o Google Drive
drive.mount('/content/drive')

# Data paths
dronedetect_raw_path = '/content/drive/MyDrive/RF-base drone detection/DroneDetect_V2'
dronedetect_feat_path = '/content/drive/MyDrive/RF-base drone detection/DroneDetect_V2_spectrogram'

# DroneDetect
main_folder = dronedetect_raw_path # data path on mp-gpu-desktop06
sub_folders = os.listdir(main_folder)

# Dataset Info
fs = 60e6 #60 MHz
bandwidth = 28e6 # 28MHz
center_freq = 2.43e9

"""# Lendo e carregando os arquivos

# Não executar novamente!
"""

import os
import pickle
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal, interpolate
from datetime import date

# Função save_spec_image_fig ajustada para aceitar todos os parâmetros corretamente
def save_spec_image_fig(folder_path, interference_type, drone_name, cond_name, fi_number, counter, fig, dpi):
    full_img_path = os.path.join(folder_path, f"{interference_type}_{drone_name}_{cond_name}_{fi_number}_{str(counter)}.jpg")
    fig.savefig(full_img_path, dpi=dpi)
    plt.close(fig)

# Garantir que o caminho seja formado corretamente
main_folder = '/content/drive/MyDrive/RF-base drone detection/DroneDetect_V2/'

# Specifications on what features to generate
n_per_seg = 1024  # length of each segment (powers of 2)
n_overlap_spec = 120
psd_win_type = 'hamming'  # make ends of each segment match
spec_han_window = np.hanning(n_per_seg)
t_seg = 20  # sample length in milliseconds
feature_to_save = ['SPEC']  # what features to generate and save: SPEC or PSD, or RAW
format_to_save = ['IMG']  # IMG or ARR
inteference_folders = ['CLEAN']  # options: ['WIFI', 'BLUE', 'BOTH', 'CLEAN']
to_add = True  # to add to existing directory

# Image properties
dim_px = (224, 224)  # dimension of image pixels
dpi = 100

# RAW data downsample: length
iq_samp_len = 10000  # 10000 is the input lengths

# Data saving folders
features_folder = '/content/drive/MyDrive/RF-base drone detection/DroneDetect_V2_spectrogramIMG_SPEC_1024_30'
date_string = date.today()
arr_spec_folder = "ARR_SPEC_" + str(n_per_seg) + "_" + str(t_seg)
arr_psd_folder = "ARR_PSD_" + str(n_per_seg) + "_" + str(t_seg)
img_spec_folder = "IMG_SPEC_" + str(n_per_seg) + "_" + str(t_seg)
img_psd_folder = "IMG_PSD_" + str(n_per_seg) + "_" + str(t_seg)
iq_folder = 'IQ_' + str(iq_samp_len) + "_" + str(t_seg)

# Check if this set of parameters already exists
if not os.path.exists(features_folder):
    os.makedirs(features_folder)

existing_folders = os.listdir(features_folder)

# Checkpoint filename
checkpoint_file = 'checkpoint.pkl'

# Load checkpoint if it exists
if os.path.exists(checkpoint_file):
    with open(checkpoint_file, 'rb') as f:
        checkpoint = pickle.load(f)
        start_sf = checkpoint.get('start_sf', 0)
        start_df = checkpoint.get('start_df', 0)
        start_fi = checkpoint.get('start_fi', 0)
else:
    checkpoint = {}
    start_sf = 0
    start_df = 0
    start_fi = 0

# Check if each of the 4 folders exist
sa_save = False  # spec array
si_save = False  # spec imag
pa_save = False  # psd array
pi_save = False  # psd imag
iq_save = False  # iq raw
if 'SPEC' in feature_to_save:
    if 'ARR' in format_to_save:
        if arr_spec_folder not in existing_folders or to_add:
            try:
                os.mkdir(os.path.join(features_folder, arr_spec_folder))
            except:
                print('folder already exist - adding')
            sa_save = True
            print('Generating SPEC in ARRAY format')
        else:
            print('Spec Arr folder already exists')
    if 'IMG' in format_to_save:
        if img_spec_folder not in existing_folders or to_add:
            try:
                os.mkdir(os.path.join(features_folder, img_spec_folder))
            except:
                print('folder already exist - adding')
            si_save = True
            print('Generating SPEC in IMAGE format')
        else:
            print('Spec Img folder already exists')
if 'PSD' in feature_to_save:
    if 'ARR' in format_to_save:
        if arr_psd_folder not in existing_folders or to_add:
            try:
                os.mkdir(os.path.join(features_folder, arr_psd_folder))
            except:
                print('folder already exist - adding')
            pa_save = True
            print('Generating PSD in ARRAY format')
        else:
            print('PSD Arr folder already exists')
    if 'IMG' in format_to_save:
        if img_psd_folder not in existing_folders or to_add:
            try:
                os.mkdir(os.path.join(features_folder, img_psd_folder))
            except:
                print('folder already exist - adding')
            pi_save = True
            print('Generating PSD in IMAGE format')
        else:
            print('PSD Img folder already exists')
if 'RAW' in feature_to_save:
    if iq_folder in existing_folders or to_add:
        try:
            os.mkdir(os.path.join(features_folder, iq_folder))
        except:
            print('RAW IQ folder already exists')
        iq_save = True

if all([not sa_save, not si_save, not pa_save, not pi_save, not iq_save]):
    print('Features Already Exist - Do Not Generate')
else:
    # Generate features
    for sf_idx, sf in enumerate(inteference_folders[start_sf:], start=start_sf):  # options: ['WIFI', 'BLUE', 'BOTH', 'CLEAN']
        print('CURRENT FOLDER: ', sf)

        drone_folders = os.listdir(main_folder + sf + '/')
        for df_idx, df in enumerate(drone_folders[start_df:], start=start_df):
            print('     subfolder:', df)
            DRONES = []
            CONDS = []
            INTS = []

            F_PSD = []
            F_SPEC = []
            F_IQ = []

            files = os.listdir(main_folder + sf + '/' + df + '/')

            # Labels (from folder name)
            drone_name = df[:3]
            cond_name = df[4:]

            for fi_idx, fi in enumerate(tqdm(files[start_fi:], initial=start_fi, total=len(files)), start=start_fi):
                # Verifique se o arquivo de saída correspondente já existe
                output_filename = f"{sf}_{fi[:-4]}_{drone_name}_{cond_name}_{fi_idx}.jpg"
                if sa_save and os.path.exists(os.path.join(features_folder, arr_spec_folder, output_filename)):
                    print(f"Pular arquivo {fi}, já processado.")
                    continue
                if pa_save and os.path.exists(os.path.join(features_folder, arr_psd_folder, output_filename)):
                    print(f"Pular arquivo {fi}, já processado.")
                    continue
                if iq_save and os.path.exists(os.path.join(features_folder, iq_folder, output_filename)):
                    print(f"Pular arquivo {fi}, já processado.")
                    continue

                d_split, _ = load_dronedetect_raw(os.path.join(main_folder, sf, df, fi), t_seg)

                # Labels (from file name)
                fi_number = fi[-6:-4]
                int_name = fi[4:6]
                for i in range(len(d_split)):  # for each split based on t_seg
                    d_complex = d_split[i]

                    # Save labels
                    DRONES.append(drone_name)
                    CONDS.append(cond_name)
                    INTS.append(int_name)

                    # Save raw data
                    if iq_save:
                        t = np.arange(0, len(d_complex))
                        f_real = interpolate.interp1d(t, d_complex.real)
                        f_imag = interpolate.interp1d(t, d_complex.imag)
                        tt = np.linspace(0, len(d_complex) - 1, num=iq_samp_len)

                        d_iq = np.stack((f_real(tt), f_imag(tt)), axis=0)
                        F_IQ.append(d_iq)

                    # Calculate PSD
                    if pa_save or pi_save:
                        fpsd, Pxx_den = signal.welch(d_complex, fs, window=psd_win_type, nperseg=n_per_seg)
                        if pa_save:
                            F_PSD.append(Pxx_den)

                        if pi_save:
                            save_psd_image(os.path.join(features_folder, img_psd_folder),
                                           drone_name, cond_name, int_name, fi_number, i, Pxx_den, dim_px, dpi)

                    # Calculate spectrogram
                    if sa_save or si_save:
                        if si_save:  # set up fig properties if saving images
                            plt.clf()
                            fig, ax = plt.subplots(1, figsize=(dim_px[0] / dpi, dim_px[1] / dpi), dpi=dpi)
                            fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
                            ax.axis('tight')
                            ax.axis('off')

                        spec, _, _, _ = plt.specgram(d_complex, NFFT=n_per_seg, Fs=fs, window=spec_han_window,
                                                     noverlap=n_overlap_spec, sides='onesided', scale='dB')

                        if si_save:
                            save_spec_image_fig(
                                folder_path=os.path.join(features_folder, img_spec_folder),
                                interference_type=sf,
                                drone_name=drone_name,
                                cond_name=cond_name,
                                fi_number=fi_number,
                                counter=i,
                                fig=fig,
                                dpi=dpi
                            )
                        if sa_save:
                            F_SPEC.append(interpolate_2d(spec, (224, 224)))

                # Save checkpoint
                checkpoint['start_sf'] = sf_idx
                checkpoint['start_df'] = df_idx
                checkpoint['start_fi'] = fi_idx
                with open(checkpoint_file, 'wb') as f:
                    pickle.dump(checkpoint, f)

            # Save data array
            if sa_save:
                save_array_detect(os.path.join(features_folder, arr_spec_folder), F_SPEC, DRONES, CONDS, INTS, 'SPEC' + "_" + df, sf, n_per_seg)
            if pa_save:
                save_array_detect(os.path.join(features_folder, arr_psd_folder), F_PSD, DRONES, CONDS, INTS, 'PSD' + "_" + df, sf, n_per_seg)
            if iq_save:
                save_array_detect(os.path.join(features_folder, iq_folder), F_IQ, DRONES, CONDS, INTS, 'RAW' + "_" + df, sf, '')

# Cleanup: Remove checkpoint if the process completes successfully
if os.path.exists(checkpoint_file):
    os.remove(checkpoint_file)

"""https://chatgpt.com/c/66de6abb-d564-8008-8652-680b99cc3578

# Lendo o conteúdo da pasta com espectogramas
"""

from google.colab import drive
import os

# Monte o Google Drive
drive.mount('/content/drive')

# Caminho correto para o dataset no Google Drive
dronedetect_feat_path = '/content/drive/MyDrive/RF-base drone detection/DroneDetect_V2_spectrogramIMG_SPEC_1024_30/IMG_SPEC_1024_20'

# Verifique se o caminho existe
if os.path.exists(dronedetect_feat_path):
    print(f"A pasta {dronedetect_feat_path} foi encontrada.")
    print("Conteúdo da pasta:")
    print(os.listdir(dronedetect_feat_path))
else:
    print(f"O caminho {dronedetect_feat_path} não existe. Verifique se a pasta está corretamente compartilhada ou montada.")

"""# Obtendo informações sobre as dimensões dos espectogramas gerados

"""

import os
from PIL import Image

# Caminho para o dataset no Google Drive
dataset_path = '/content/drive/MyDrive/RF-base drone detection/DroneDetect_V2_spectrogramIMG_SPEC_1024_30/IMG_SPEC_1024_20'

# Lista para armazenar os tamanhos das imagens
image_sizes = []

# Contar o número total de arquivos de imagem
image_files = [file for file in os.listdir(dataset_path) if file.endswith(('png', 'jpg', 'jpeg'))]

# Imprimir o número total de arquivos de imagem
print(f"Total de arquivos de imagem: {len(image_files)}")

# Iterar sobre os arquivos de imagem para obter seus tamanhos
for file in image_files:
    image_path = os.path.join(dataset_path, file)
    with Image.open(image_path) as img:
        image_sizes.append(img.size)  # img.size retorna (largura, altura)

# Imprimir o tamanho de cada imagem
for idx, size in enumerate(image_sizes):
    print(f"Tamanho da imagem {idx+1}: {size}")

"""contexto: https://chatgpt.com/c/53180f45-92b0-4a52-9133-b1961f570a89"""

import os
from PIL import Image
import torch
import numpy as np

# Caminho para o dataset no Google Drive
dataset_path = '/content/drive/MyDrive/RF-base drone detection/DroneDetect_V2_spectrogramIMG_SPEC_1024_30/IMG_SPEC_1024_20'

# Caminho para salvar as imagens
save_dir = '/content/drive/MyDrive/espectrogramas - VGGRESNET'

# Criar a pasta se ela não existir
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

# Listar todos os arquivos de imagem no diretório
image_files = [file for file in os.listdir(dataset_path) if file.endswith(('png', 'jpg', 'jpeg'))]

# Verificar se há arquivos no diretório
if len(image_files) > 0:
    for idx, file in enumerate(image_files):
        # Caminho para o arquivo de imagem
        image_path = os.path.join(dataset_path, file)

        # Carregar a imagem usando PIL
        with Image.open(image_path) as img:
            # Converter a imagem para um tensor
            image_tensor = torch.tensor(np.array(img))

            # Imprimir o índice, shape e dtype da imagem
            print(f"Imagem {idx}:")
            print(f"Shape: {image_tensor.shape}")
            print(f"Dtype: {image_tensor.dtype}")
            print('-' * 30)

            # Salvar a imagem no novo diretório
            img_save_path = os.path.join(save_dir, f"processed_{file}")
            img.save(img_save_path)
            print(f"Imagem {idx} salva em {img_save_path}")
else:
    print(f"O diretório não contém arquivos de imagem.")

"""17 minutos"""

import os
from PIL import Image
import torch
import numpy as np

# Caminho para o dataset no Google Drive
dataset_path = '/content/drive/MyDrive/RF-base drone detection/DroneDetect_V2_spectrogramIMG_SPEC_1024_30/IMG_SPEC_1024_20'

# Listar todos os arquivos de imagem no diretório
image_files = [file for file in os.listdir(dataset_path) if file.endswith(('png', 'jpg', 'jpeg'))]

# Verificar se há arquivos no diretório
if len(image_files) > 0:
    for idx, file in enumerate(image_files):
        # Caminho para o arquivo de imagem
        image_path = os.path.join(dataset_path, file)

        # Carregar a imagem usando PIL
        with Image.open(image_path) as img:
            # Converter a imagem para um tensor
            image_tensor = torch.tensor(np.array(img))

            # Imprimir o índice, shape e dtype da imagem
            print(f"Imagem {idx}:")
            print(f"Shape: {image_tensor.shape}")
            print(f"Dtype: {image_tensor.dtype}")
            print('-' * 30)
else:
    print(f"O diretório não contém arquivos de imagem.")

import os
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

# Caminho para o dataset no Google Drive
dataset_path = '/content/drive/MyDrive/RF-base drone detection/DroneDetect_V2_spectrogramIMG_SPEC_1024_30/IMG_SPEC_1024_20'

# Listar todos os arquivos de imagem no diretório
image_files = [file for file in os.listdir(dataset_path) if file.endswith(('png', 'jpg', 'jpeg'))]

# Verifique se há pelo menos 91 arquivos no diretório
if len(image_files) > 90:
    # Caminho para o arquivo de imagem no índice 90
    image_path = os.path.join(dataset_path, image_files[9470])

    # Carregar a imagem usando PIL
    with Image.open(image_path) as img:
        # Converter a imagem para um array numpy
        image_array = np.array(img)

        # Exibir a imagem usando matplotlib
        plt.imshow(image_array)
        plt.title(f'Imagem no índice 9470: {image_files[9470]}')
        plt.axis('off')  # Desliga os eixos
        plt.show()
else:
    print(f"O diretório contém menos de 91 arquivos de imagem.")

Model.isarray = False

import os
from collections import Counter
from PIL import Image
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

# Caminho para o dataset no Google Drive
dataset_path = '/content/drive/MyDrive/RF-base drone detection/DroneDetect_V2_spectrogramIMG_SPEC_1024_30/IMG_SPEC_1024_20'

# Transformações para as imagens
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Redimensiona para o tamanho esperado pelo modelo
    transforms.ToTensor()  # Converte a imagem para tensor PyTorch
])

# Custom Dataset
class CustomImageDataset(Dataset):
    def __init__(self, img_dir, transform=None):
        self.img_dir = img_dir
        self.transform = transform
        self.img_labels = []
        self.img_paths = []

        # Percorrer todos os arquivos na pasta e extrair as classes e caminhos
        for file_name in os.listdir(img_dir):
            if file_name.endswith(('png', 'jpg', 'jpeg')):
                path = os.path.join(img_dir, file_name)
                # Supondo que a classe é o segundo elemento no nome do arquivo, separado por '_'
                label = file_name.split('_')[1]  # extrai drone_name
                self.img_labels.append(label)
                self.img_paths.append(path)

        # Criar um dicionário de classe para índice
        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(sorted(set(self.img_labels)))}
        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        image = Image.open(img_path).convert('RGB')
        label = self.class_to_idx[self.img_labels[idx]]
        if self.transform:
            image = self.transform(image)
        return image, label

# Criar o dataset customizado
dataset = CustomImageDataset(img_dir=dataset_path, transform=transform)

# Exibir as classes disponíveis
print("Classes disponíveis:", dataset.class_to_idx)

# Divisão estratificada do dataset em treino, validação e teste
indices = list(range(len(dataset)))
labels = [dataset.img_labels[i] for i in indices]

train_indices, temp_indices, _, temp_labels = train_test_split(
    indices, labels, stratify=labels, test_size=(1 - 0.75), random_state=42)

val_indices, test_indices = train_test_split(
    temp_indices, stratify=temp_labels, test_size=(0.2 / (0.2 + 0.05)), random_state=42)

train_dataset = Subset(dataset, train_indices)
validation_dataset = Subset(dataset, val_indices)
test_dataset = Subset(dataset, test_indices)

# Função para contar as ocorrências de cada classe
def count_classes(dataset, class_to_idx):
    labels = [dataset.dataset.img_labels[i] for i in dataset.indices]
    label_counts = Counter(labels)
    # Mapear índices para nomes de classes
    return {class_to_idx[label]: count for label, count in label_counts.items()}

# Contar as classes em cada conjunto
train_class_counts = count_classes(train_dataset, dataset.class_to_idx)
val_class_counts = count_classes(validation_dataset, dataset.class_to_idx)
test_class_counts = count_classes(test_dataset, dataset.class_to_idx)

# Função para plotar gráfico de barras com percentuais e quantidades
def plot_class_distribution(train_counts, val_counts, test_counts, title):
    classes = sorted(train_counts.keys())
    total_counts = {cls: train_counts.get(cls, 0) + val_counts.get(cls, 0) + test_counts.get(cls, 0) for cls in classes}

    train_values = [train_counts.get(cls, 0) for cls in classes]
    val_values = [val_counts.get(cls, 0) for cls in classes]
    test_values = [test_counts.get(cls, 0) for cls in classes]

    bar_width = 0.2
    r1 = np.arange(len(classes))
    r2 = [x + bar_width for x in r1]
    r3 = [x + bar_width for x in r2]

    plt.figure(figsize=(12, 6))
    bars1 = plt.bar(r1, train_values, color='blue', width=bar_width, edgecolor='grey', label='Treino')
    bars2 = plt.bar(r2, val_values, color='green', width=bar_width, edgecolor='grey', label='Validação')
    bars3 = plt.bar(r3, test_values, color='red', width=bar_width, edgecolor='grey', label='Teste')

    plt.xlabel('Classes', fontweight='bold')
    plt.ylabel('Número de Amostras', fontweight='bold')
    plt.title(title)
    plt.xticks([r + bar_width for r in range(len(classes))], [dataset.idx_to_class[cls] for cls in classes])
    plt.legend()

    # Adicionar percentuais e quantidades em cima de cada barra
    for bars, counts in zip([bars1, bars2, bars3], [train_values, val_values, test_values]):
        for bar, count, cls in zip(bars, counts, classes):
            percentage = 100 * count / total_counts[cls]
            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{count} ({percentage:.1f}%)',
                     ha='center', va='bottom', fontsize=10, fontweight='bold')

    plt.show()

# Plotar a distribuição das classes
plot_class_distribution(train_class_counts, val_class_counts, test_class_counts, "Distribuição das Classes nos Conjuntos de Treino, Validação e Teste")

# Configurações do DataLoader
num_workers = 19
batch_size = 64

# DataLoaders para treino, validação e teste
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
validation_dataloader = DataLoader(validation_dataset, batch_size=1, num_workers=num_workers)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)

print(f"Tamanho do dataset de treino: {len(train_dataset)}")
print(f"Tamanho do dataset de validação: {len(validation_dataset)}")
print(f"Tamanho do dataset de teste: {len(test_dataset)}")

"""contexto: https://chatgpt.com/c/53180f45-92b0-4a52-9133-b1961f570a89"""

import os
from collections import Counter
from PIL import Image
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
import numpy as np
import torch

# Caminho para o dataset no Google Drive
dataset_path = '/content/drive/MyDrive/RF-base drone detection/DroneDetect_V2_spectrogramIMG_SPEC_1024_30/IMG_SPEC_1024_20'

# Transformações para as imagens
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Redimensiona para o tamanho esperado pelo modelo
    transforms.ToTensor()  # Converte a imagem para tensor PyTorch
])

# Custom Dataset
class CustomImageDataset(Dataset):
    def __init__(self, img_dir, transform=None):
        self.img_dir = img_dir
        self.transform = transform
        self.img_labels = []
        self.img_paths = []

        for file_name in os.listdir(img_dir):
            if file_name.endswith(('png', 'jpg', 'jpeg')):
                path = os.path.join(img_dir, file_name)
                label = file_name.split('_')[1]  # extrai drone_name
                self.img_labels.append(label)
                self.img_paths.append(path)

        # Criar um dicionário de classe para índice
        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(sorted(set(self.img_labels)))}
        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        image = Image.open(img_path).convert('RGB')
        label = self.class_to_idx[self.img_labels[idx]]
        if self.transform:
            image = self.transform(image)
        return image, label

# Criar o dataset customizado
dataset = CustomImageDataset(img_dir=dataset_path, transform=transform)

"""VGG https://medium.com/@mygreatlearning/everything-you-need-to-know-about-vgg16-7315defb5918"""

import torch
import torch.nn as nn
from torchvision import models

# Definir a classe ResNetFC
class ResNetFC(nn.Module):
    def __init__(self, num_classes):
        super(ResNetFC, self).__init__()
        self.num_classes = num_classes
        self.resnetfull = models.resnet50(pretrained=True)

        # Remover as camadas totalmente conectadas e de pooling da ResNet
        modules = list(self.resnetfull.children())[:-2]  # Remove a fully connected layer e adaptive averaging
        self.resnetfeats = nn.Sequential(*modules)

        # Congelar os parâmetros da rede ResNet
        for param in self.resnetfeats.parameters():
            param.requires_grad = False

        # Adicionar uma nova camada totalmente conectada
        self._fc = nn.Linear(100352, num_classes)  # Ajustar para o tamanho da saída da ResNet

    def forward(self, x):
        if len(x.shape) == 4:
            x = torch.moveaxis(x, -1, 1)  # Corrigir permutação se a imagem estiver no formato [B, H, W, C]

        # Passar pelas camadas convolucionais da ResNet
        x = self.resnetfeats(x)
        x = x.view(x.size(0), -1)  # Achatar o tensor antes da camada totalmente conectada
        x = self._fc(x)

        return x

    def reset_weights(self):
        print(f'Reset trainable parameters of layer = {self._fc}')
        self._fc.reset_parameters()

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import KFold
from torchvision import models

# Definir a classe ResNetFC conforme solicitado
class ResNetFC(nn.Module):
    def __init__(self, num_classes):
        super(ResNetFC, self).__init__()
        self.num_classes = num_classes
        self.resnetfull = models.resnet50(weights='IMAGENET1K_V1')

        # Remover as camadas totalmente conectadas e de pooling da ResNet
        modules = list(self.resnetfull.children())[:-2]
        self.resnetfeats = nn.Sequential(*modules)

        # Congelar os parâmetros da rede ResNet
        for param in self.resnetfeats.parameters():
            param.requires_grad = False

        # Adicionar uma nova camada totalmente conectada
        self._fc = nn.Linear(100352, num_classes)

    def forward(self, x):
        # Garantir que as dimensões estejam na ordem correta
        if len(x.shape) == 4 and x.shape[1] != 3:
            x = x.permute(0, 3, 1, 2)  # Corrigir permutação para [batch_size, channels, height, width]

        # Passar pelas camadas convolucionais da ResNet
        x = self.resnetfeats(x)
        x = x.view(x.size(0), -1)  # Achatar o tensor antes da camada totalmente conectada
        x = self._fc(x)

        return x

    def reset_weights(self):
        print(f'Reset trainable parameters of layer = {self._fc}')
        self._fc.reset_parameters()

# Configurações do treinamento
k_folds = 2
batch_size = 128
learning_rate = 0.01
num_epochs = 200
momentum = 0.95
l2reg = 1e-4
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Inicializar o modelo ResNetFC
model = ResNetFC(num_classes=len(dataset.class_to_idx)).to(device)

# Critério de perda
criterion = nn.CrossEntropyLoss()

# Função para treinar o modelo
def train_model(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    for images, labels in train_loader:
        # Garantir que as dimensões da imagem estejam corretas antes de passar para o modelo
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    return running_loss / len(train_loader)

# Função para avaliar o modelo
def evaluate_model(model, val_loader, criterion, device):
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in val_loader:
            # Garantir que as dimensões da imagem estejam corretas antes de passar para o modelo
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    val_loss /= len(val_loader)
    val_acc = correct / total
    return val_loss, val_acc

# K-Fold Cross-Validation
kf = KFold(n_splits=k_folds, shuffle=True)
results = []

# Variáveis para armazenar perdas e acurácias por época
all_train_losses = []
all_val_losses = []
all_val_accuracies = []

for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):
    print(f'Fold {fold+1}/{k_folds}')

    train_subset = Subset(dataset, train_idx)
    val_subset = Subset(dataset, val_idx)

    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)

    # Otimizador
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2reg)

    train_losses = []
    val_losses = []
    val_accuracies = []

    for epoch in range(num_epochs):
        train_loss = train_model(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)
        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')

        # Armazenar as métricas
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)

    all_train_losses.append(train_losses)
    all_val_losses.append(val_losses)
    all_val_accuracies.append(val_accuracies)

    results.append(val_acc)

print(f'K-Fold Cross-Validation Results: {results}')
print(f'Mean Accuracy: {np.mean(results):.4f}')

"""FAZER CROSS VALIDATION LÁ EM CIMA

Código abaixo: 7 horas
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import KFold

# Configurações do treinamento
k_folds = 5
batch_size = 128
learning_rate = 0.01
num_epochs = 100
momentum = 0.95
l2reg = 1e-4
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Inicializar o modelo
model = VGGFC(num_classes=len(dataset.class_to_idx)).to(device)

# Critério de perda
criterion = nn.CrossEntropyLoss()

# Função para treinar o modelo
def train_model(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    return running_loss / len(train_loader)

# Função para avaliar o modelo
def evaluate_model(model, val_loader, criterion, device):
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    val_loss /= len(val_loader)
    val_acc = correct / total
    return val_loss, val_acc

# K-Fold Cross-Validation
kf = KFold(n_splits=k_folds, shuffle=True)
results = []

# Variáveis para armazenar perdas e acurácias por época
all_train_losses = []
all_val_losses = []
all_val_accuracies = []

for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):
    print(f'Fold {fold+1}/{k_folds}')

    train_subset = Subset(dataset, train_idx)
    val_subset = Subset(dataset, val_idx)

    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)

    # Otimizador
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2reg)

    train_losses = []
    val_losses = []
    val_accuracies = []

    for epoch in range(num_epochs):
        train_loss = train_model(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)
        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')

        # Armazenar as métricas
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)

    all_train_losses.append(train_losses)
    all_val_losses.append(val_losses)
    all_val_accuracies.append(val_accuracies)

    results.append(val_acc)

print(f'K-Fold Cross-Validation Results: {results}')
print(f'Mean Accuracy: {np.mean(results):.4f}')

# --- Plotar os gráficos de loss e acurácia por epoch ---

# Gráfico de Loss x Epoch
plt.figure(figsize=(10, 6))
for fold_idx, losses in enumerate(all_train_losses):
    plt.plot(range(1, num_epochs + 1), losses, label=f'Train Loss Fold {fold_idx + 1}')
for fold_idx, losses in enumerate(all_val_losses):
    plt.plot(range(1, num_epochs + 1), losses, '--', label=f'Val Loss Fold {fold_idx + 1}')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss by Epoch')
plt.legend()
plt.show()

# Gráfico de Acurácia x Epoch
plt.figure(figsize=(10, 6))
for fold_idx, accuracies in enumerate(all_val_accuracies):
    plt.plot(range(1, num_epochs + 1), accuracies, label=f'Val Accuracy Fold {fold_idx + 1}')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Validation Accuracy by Epoch')
plt.legend()
plt.show()

"""# AVALIAÇÃO DO MODELO"""

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns

# Avaliação final do modelo com o conjunto de teste
def test_model(model, test_loader, device):
    model.eval()
    all_labels = []
    all_preds = []

    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            _, predicted = torch.max(outputs, 1)

            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())

    return all_labels, all_preds

# Carregar o DataLoader do conjunto de teste (usando o último split de validação para treinar o modelo final)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

# Executar a avaliação no conjunto de teste
test_labels, test_preds = test_model(model, test_loader, device)

# Calcular métricas
precision = precision_score(test_labels, test_preds, average='weighted')
recall = recall_score(test_labels, test_preds, average='weighted')
f1 = f1_score(test_labels, test_preds, average='weighted')
conf_matrix = confusion_matrix(test_labels, test_preds)

print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')

# Imprimir a matriz de confusão
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=dataset.idx_to_class.values(), yticklabels=dataset.idx_to_class.values())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""https://colab.research.google.com/#scrollTo=54e3bfc9&fileId=https%3A//dagshub.com/ltindall/RFClassification/raw/08718d14088e48d9bfa5d15611910fe45676155d/DL%2520Approaches.ipynb

https://dagshub.com/ltindall/RFClassification
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import KFold
from tabulate import tabulate

# Configurações do treinamento
k_folds = 5
batch_size = 256
learning_rate = 0.01
num_epochs = 10
momentum = 0.95
l2reg = 1e-4
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Inicializar o modelo
model = VGGFC(num_classes=len(dataset.class_to_idx)).to(device)

# Critério de perda
criterion = nn.CrossEntropyLoss()

# Função para treinar o modelo
def train_model(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    return running_loss / len(train_loader)

# Função para avaliar o modelo
def evaluate_model(model, val_loader, criterion, device):
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    val_loss /= len(val_loader)
    val_acc = correct / total
    return val_loss, val_acc

# K-Fold Cross-Validation
kf = KFold(n_splits=k_folds, shuffle=True)
results = []

# Variáveis para armazenar perdas e acurácias por época
all_train_losses = []
all_val_losses = []
all_val_accuracies = []

# Para a tabela
fold_metrics = []

for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):
    print(f'Fold {fold+1}/{k_folds}')

    train_subset = Subset(dataset, train_idx)
    val_subset = Subset(dataset, val_idx)

    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)

    # Otimizador
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2reg)

    train_losses = []
    val_losses = []
    val_accuracies = []

    for epoch in range(num_epochs):
        train_loss = train_model(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)
        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')

        # Armazenar as métricas
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)

    all_train_losses.append(train_losses)
    all_val_losses.append(val_losses)
    all_val_accuracies.append(val_accuracies)

    fold_avg_val_loss = np.mean(val_losses)
    fold_avg_val_acc = np.mean(val_accuracies)

    fold_metrics.append([fold+1, fold_avg_val_loss, fold_avg_val_acc])
    results.append(fold_avg_val_acc)

# Tabela com as métricas
header = ['Fold', 'Avg Val Loss', 'Avg Val Accuracy']
fold_metrics.append(['Mean', np.mean([x[1] for x in fold_metrics]), np.mean(results)])
print(tabulate(fold_metrics, headers=header, floatfmt=".4f"))

# --- Plotar os gráficos por k-fold ---

for fold_idx in range(k_folds):
    # Gráfico de perda (Loss) por época para cada fold
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, num_epochs + 1), all_train_losses[fold_idx], label=f'Train Loss Fold {fold_idx + 1}')
    plt.plot(range(1, num_epochs + 1), all_val_losses[fold_idx], '--', label=f'Val Loss Fold {fold_idx + 1}')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title(f'Fold {fold_idx + 1} - Training and Validation Loss by Epoch')
    plt.legend()
    plt.show()

    # Gráfico de acurácia (Accuracy) por época para cada fold
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, num_epochs + 1), all_val_accuracies[fold_idx], label=f'Val Accuracy Fold {fold_idx + 1}')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title(f'Fold {fold_idx + 1} - Validation Accuracy by Epoch')
    plt.legend()
    plt.show()

# Imprimir os resultados finais
print(f'K-Fold Cross-Validation Results: {results}')
print(f'Mean Accuracy: {np.mean(results):.4f}')

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns

# Avaliação final do modelo com o conjunto de teste
def test_model(model, test_loader, device):
    model.eval()
    all_labels = []
    all_preds = []

    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            _, predicted = torch.max(outputs, 1)

            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())

    return all_labels, all_preds

# Carregar o DataLoader do conjunto de teste (usando o último split de validação para treinar o modelo final)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

# Executar a avaliação no conjunto de teste
test_labels, test_preds = test_model(model, test_loader, device)

# Calcular métricas
precision = precision_score(test_labels, test_preds, average='weighted')
recall = recall_score(test_labels, test_preds, average='weighted')
f1 = f1_score(test_labels, test_preds, average='weighted')
conf_matrix = confusion_matrix(test_labels, test_preds)

print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')

# Imprimir a matriz de confusão
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=dataset.idx_to_class.values(), yticklabels=dataset.idx_to_class.values())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""# CODIGO COMPLETO"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import KFold
from tabulate import tabulate
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns

# Função para treinar o modelo
def train_model(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    return running_loss / len(train_loader)

# Função para avaliar o modelo
def evaluate_model(model, val_loader, criterion, device):
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    val_loss /= len(val_loader)
    val_acc = correct / total
    return val_loss, val_acc

# Função para testar o modelo
def test_model(model, test_loader, device):
    model.eval()
    all_labels = []
    all_preds = []

    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            _, predicted = torch.max(outputs, 1)

            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())

    return all_labels, all_preds

# Função para executar e avaliar cada combinação de hiperparâmetros
def run_experiment(k_folds, batch_size, num_epochs):
    print(f'Running experiment with k_folds={k_folds}, batch_size={batch_size}, num_epochs={num_epochs}')

    # Critério de perda
    criterion = nn.CrossEntropyLoss()

    # K-Fold Cross-Validation
    kf = KFold(n_splits=k_folds, shuffle=True)
    results = []

    # Variáveis para armazenar perdas e acurácias por época
    all_train_losses = []
    all_val_losses = []
    all_val_accuracies = []

    # Para a tabela
    fold_metrics = []

    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):
        print(f'Fold {fold+1}/{k_folds}')

        train_subset = Subset(dataset, train_idx)
        val_subset = Subset(dataset, val_idx)

        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)

        # Inicializar o modelo e o otimizador
        model = VGGFC(num_classes=len(dataset.class_to_idx)).to(device)
        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2reg)

        train_losses = []
        val_losses = []
        val_accuracies = []

        for epoch in range(num_epochs):
            train_loss = train_model(model, train_loader, criterion, optimizer, device)
            val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)
            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')

            # Armazenar as métricas
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            val_accuracies.append(val_acc)

        all_train_losses.append(train_losses)
        all_val_losses.append(val_losses)
        all_val_accuracies.append(val_accuracies)

        fold_avg_val_loss = np.mean(val_losses)
        fold_avg_val_acc = np.mean(val_accuracies)

        fold_metrics.append([fold+1, fold_avg_val_loss, fold_avg_val_acc])
        results.append(fold_avg_val_acc)

    # Plotar gráficos de perda e acurácia por fold
    for fold_idx in range(k_folds):
        plt.figure(figsize=(10, 5))
        plt.plot(range(1, num_epochs + 1), all_train_losses[fold_idx], label=f'Train Loss Fold {fold_idx + 1}')
        plt.plot(range(1, num_epochs + 1), all_val_losses[fold_idx], '--', label=f'Val Loss Fold {fold_idx + 1}')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.title(f'Fold {fold_idx + 1} - Training and Validation Loss by Epoch')
        plt.legend()
        plt.show()

        plt.figure(figsize=(10, 5))
        plt.plot(range(1, num_epochs + 1), all_val_accuracies[fold_idx], label=f'Val Accuracy Fold {fold_idx + 1}')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.title(f'Fold {fold_idx + 1} - Validation Accuracy by Epoch')
        plt.legend()
        plt.show()

    # Tabela com as métricas
    header = ['Fold', 'Avg Val Loss', 'Avg Val Accuracy']
    fold_metrics.append(['Mean', np.mean([x[1] for x in fold_metrics]), np.mean(results)])
    print(tabulate(fold_metrics, headers=header, floatfmt=".4f"))

    return np.mean(results), model  # Retorna o modelo treinado também

# Configurações dos hiperparâmetros a serem testados
k_folds_list = [10]
batch_size_list = [64, 128]
num_epochs_list = [50]

# Para armazenar resultados de todas as combinações
combination_results = []

# Executar todas as combinações
for k_folds in k_folds_list:
    for batch_size in batch_size_list:
        for num_epochs in num_epochs_list:
            mean_accuracy, _ = run_experiment(k_folds, batch_size, num_epochs)
            combination_results.append([k_folds, batch_size, num_epochs, mean_accuracy])

# Exibir os resultados de todas as combinações
header = ['K-Folds', 'Batch Size', 'Num Epochs', 'Mean Accuracy']
print(tabulate(combination_results, headers=header, floatfmt=".4f"))

# Encontrar e imprimir a melhor combinação de hiperparâmetros
best_combination = max(combination_results, key=lambda x: x[3])
print(f"\nBest combination: K-Folds = {best_combination[0]}, Batch Size = {best_combination[1]}, Num Epochs = {best_combination[2]} with Mean Accuracy = {best_combination[3]:.4f}")

# Treinar o modelo final com a melhor combinação
best_k_folds, best_batch_size, best_num_epochs = best_combination[0], best_combination[1], best_combination[2]
_, best_model = run_experiment(best_k_folds, best_batch_size, best_num_epochs)

# Avaliação final no conjunto de teste
test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False, num_workers=4)  # Altere `num_workers` conforme necessário
test_labels, test_preds = test_model(best_model, test_loader, device)

# Calcular métricas de teste
precision = precision_score(test_labels, test_preds, average='weighted')
recall = recall_score(test_labels, test_preds, average='weighted')
f1 = f1_score(test_labels, test_preds, average='weighted')
conf_matrix = confusion_matrix(test_labels, test_preds)

print(f'Test Precision: {precision:.4f}')
print(f'Test Recall: {recall:.4f}')
print(f'Test F1 Score: {f1:.4f}')

# Imprimir a matriz de confusão
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=dataset.idx_to_class.values(), yticklabels=dataset.idx_to_class.values())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()





































"""# ANTIGO"""

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns

# Avaliação final do modelo com o conjunto de teste
def test_model(model, test_loader, device):
    model.eval()
    all_labels = []
    all_preds = []

    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            _, predicted = torch.max(outputs, 1)

            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())

    return all_labels, all_preds

# Carregar o DataLoader do conjunto de teste (usando o último split de validação para treinar o modelo final)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

# Executar a avaliação no conjunto de teste
test_labels, test_preds = test_model(model, test_loader, device)

# Calcular métricas
precision = precision_score(test_labels, test_preds, average='weighted')
recall = recall_score(test_labels, test_preds, average='weighted')
f1 = f1_score(test_labels, test_preds, average='weighted')
conf_matrix = confusion_matrix(test_labels, test_preds)

print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')

# Imprimir a matriz de confusão
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=dataset.idx_to_class.values(), yticklabels=dataset.idx_to_class.values())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()